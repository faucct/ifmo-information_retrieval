# Отчёт

## Первичная обработка данных

Для чтения XML и выделения ссылок из HTML использовался пакет `lxml`, а для выделения текста из HTML – `html_text`.

Каждая партиция обрабатывалась параллельно. Результаты предобработки были экспортированы в два вида файлов, каждые строчки которых соответствовали документам из исходных партиций и содержали:
* текстовые данные, закодированные в UTF-8 + Base64
* ссылки, закодированные в UTF-8 + Base64 и разделённые символом ','

Предобработка занимает примерно десять минут.

![Byte lengths](byte_lengths.png)

Кодировка текста – Windows-1251. Незнакомые символы считались как 1 байт.

![Word lengths](word_lengths.png)

Пробельные символы разбивали текст на слова.

![Text to HTML ratios](html_text_ratios.png)

## Граф гиперссылок

Помимо ссылок со страницы при первичной обработке находилась ссылка на саму страницу.
Затем с её помощью брались абсолютные ссылки. И после записывались в строку через ',' где первыой стоит ссылка на саму страницу.

После по всем партициям были найденны n самых популярных странниц, т.е. страниц с наибольшим числом ссылок на них. 
Все остальные странницы выкидывались из рассмотрения и граф ссылок строился только для саммых популярных странниц. 
Это было сделанно т.к. граф для всех 200000 страниц был бы не читаем. 
В качестве n было выбранно 500 и 1000.
 
![500 urls link graph](url500.png)

![1000 urls link graph](url1000.png)

Как видно многие страницы образуют отдельные компоненты связности, которые соответсвуют различным интернет порталам.
